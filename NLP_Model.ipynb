{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.notebook import tqdm  # Importing the notebook version of tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "# Load a dataset from the internet. For example, you might use the Reddit Self-reported Depression Diagnosis Dataset from Kaggle.\n",
    "# You would have to download it and upload to your environment. Here's a basic loading example:\n",
    "df = pd.read_csv('mental_health.csv')  # Replace with your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# Custom dataset for our data loading\n",
    "class MentalHealthDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# In[4]:\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Binary classification\n",
    "\n",
    "# Splitting dataset into training and validation\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "train_df, val_df = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "train_dataset = MentalHealthDataset(train_df['text'].values, train_df['label'].values, tokenizer, 256)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "val_dataset = MentalHealthDataset(val_df['text'].values, val_df['label'].values, tokenizer, 256)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saura\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# In[5]:\n",
    "# Model training setup\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary Cross-Entropy\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # Wrapping the data_loader with tqdm to show the progress bar\n",
    "    for batch in tqdm(data_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].float().unsqueeze(1).to(device)  # Adjust for BCE loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171d6af0bac54eadb9fb449ae239af57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1399 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\saura\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.1668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21dfd8e75c14c1a86ac449463acb2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1399 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21575a5da5848908da5aeec6d3ffcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1399 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0377\n"
     ]
    }
   ],
   "source": [
    "# In[6]:\n",
    "# Model training loop\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 96.18%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].float().unsqueeze(1).to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = (torch.sigmoid(outputs.logits) > 0.5).float()  # Assuming a threshold of 0.5 for binary classification\n",
    "\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct_predictions / total\n",
    "\n",
    "val_accuracy = evaluate(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "# Evaluation\n",
    "def predict(model, text, tokenizer, device):\n",
    "    model = model.eval()\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        prediction = torch.sigmoid(outputs.logits)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "# Risk prediction\n",
    "def risk_score(text, model, tokenizer, device):\n",
    "    score = predict(model, text, tokenizer, device)\n",
    "    # Rescale to [1,10]\n",
    "    return 1 + 9 * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "# Trigger word flagging\n",
    "trigger_words = ['suicide', 'kill', 'death', 'die', 'end my life', 'hurt myself']\n",
    "def has_trigger_words(text):\n",
    "    for word in trigger_words:\n",
    "        if word in text:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saura\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of depression/suicidal tendencies: 0.52\n",
      "Risk score (1-10): 5.70\n",
      "This text contains no triggering words\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"I've been feeling really down lately.\"\n",
    "\n",
    "# Predict depression/suicidal tendencies probability\n",
    "prediction_probability = predict(model, sample_text, tokenizer, device)\n",
    "print(f\"Probability of depression/suicidal tendencies: {prediction_probability:.2f}\")\n",
    "\n",
    "# Calculate risk score\n",
    "risk = risk_score(sample_text, model, tokenizer, device)\n",
    "print(f\"Risk score (1-10): {risk:.2f}\")\n",
    "\n",
    "# Check for triggering words\n",
    "if has_trigger_words(sample_text):\n",
    "    print(\"Warning: The text contains triggering words.\")\n",
    "else:\n",
    "    print(\"This text contains no triggering words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saura\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      2827\n",
      "           1       0.96      0.96      0.96      2769\n",
      "\n",
      "    accuracy                           0.96      5596\n",
      "   macro avg       0.96      0.96      0.96      5596\n",
      "weighted avg       0.96      0.96      0.96      5596\n",
      "\n",
      "Precision: 0.9627\n",
      "Recall: 0.9599\n",
      "F1 Score: 0.9613\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract true labels from DataLoader\n",
    "def extract_true_labels(data_loader):\n",
    "    true_labels = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        labels = batch['label'].numpy()\n",
    "        true_labels.extend(labels)\n",
    "    \n",
    "    return true_labels\n",
    "\n",
    "# Extract true labels for validation data\n",
    "labels_val_true = extract_true_labels(val_loader)\n",
    "\n",
    "# Now, use these labels for evaluation\n",
    "# Option 1: Using classification_report\n",
    "print(classification_report(labels_val_true, labels_pred))\n",
    "\n",
    "# Option 2: Using individual functions\n",
    "precision = precision_score(labels_val_true, labels_pred)\n",
    "recall = recall_score(labels_val_true, labels_pred)\n",
    "f1 = f1_score(labels_val_true, labels_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "\n",
    "def get_results():\n",
    "    # Extracting text from the GUI input field\n",
    "    sample_text = text_input.get(\"1.0\", \"end-1c\")\n",
    "    \n",
    "    # Predict depression/suicidal tendencies probability\n",
    "    prediction_probability = predict(model, sample_text, tokenizer, device)\n",
    "    # Calculate risk score\n",
    "    risk = risk_score(sample_text, model, tokenizer, device)\n",
    "    \n",
    "    # Update the canvas\n",
    "    draw_severity_indicator(risk)\n",
    "    \n",
    "    # Trigger words check\n",
    "    if has_trigger_words(sample_text):\n",
    "        trigger_label[\"text\"] = \"Warning: The text contains triggering words.\"\n",
    "        trigger_label.config(fg='red')\n",
    "    else:\n",
    "        trigger_label[\"text\"] = \"This text contains no triggering words.\"\n",
    "        trigger_label.config(fg='black')\n",
    "\n",
    "def draw_severity_indicator(risk):\n",
    "    # Pie chart\n",
    "    fig = Figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    data = [risk, 10-risk]\n",
    "    labels = ['Risk Score', 'Remaining']\n",
    "    colors = ['red', 'green'] if risk > 5 else ['yellow', 'green'] if risk > 3 else ['green', 'lightgreen']\n",
    "    explode = [0.1, 0]\n",
    "\n",
    "    ax.pie(data, labels=labels, colors=colors, explode=explode, autopct='%1.1f%%', startangle=140)\n",
    "    ax.set_title(f\"Risk Score: {risk:.2f}\")\n",
    "\n",
    "    # Embed the chart into tkinter\n",
    "    chart = FigureCanvasTkAgg(fig, master=chart_frame)\n",
    "    chart.draw()\n",
    "    chart.get_tk_widget().pack(pady=15)\n",
    "\n",
    "# GUI setup\n",
    "app = tk.Tk()\n",
    "app.title(\"Mental Health Analysis\")\n",
    "\n",
    "main_frame = ttk.Frame(app)\n",
    "main_frame.pack(padx=20, pady=20)\n",
    "\n",
    "# Label and Text input field for sample text\n",
    "text_frame = ttk.LabelFrame(main_frame, text=\"Input Text\")\n",
    "text_frame.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "text_input = tk.Text(text_frame, height=5, width=50)\n",
    "text_input.pack(padx=5, pady=5)\n",
    "\n",
    "# Button to compute results\n",
    "compute_button = ttk.Button(text_frame, text=\"Analyze\", command=get_results)\n",
    "compute_button.pack(pady=15)\n",
    "\n",
    "# Results frame\n",
    "results_frame = ttk.LabelFrame(main_frame, text=\"Results\")\n",
    "results_frame.grid(row=0, column=1, padx=10, pady=10)\n",
    "\n",
    "trigger_label = ttk.Label(results_frame, text=\"\", font=('Arial', 12))\n",
    "trigger_label.pack(pady=10)\n",
    "\n",
    "# Chart frame\n",
    "chart_frame = ttk.Frame(results_frame)\n",
    "chart_frame.pack(pady=15)\n",
    "\n",
    "app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "962446a72a229e32d525ed7bb38298bd9954d83b318af501cdbecdd3b70143a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
